{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\" Hello everyone, This is Natural Language Processing Laboratory, Natural Language Processing is branch of Artificial Intelligence\"]\n",
    "tokens = [[item for item in line.split()] for line in text1]\n",
    "g_dict1 = corpora.Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 12 tokens\n",
      "\n",
      "{'Artificial': 0, 'Hello': 1, 'Intelligence': 2, 'Laboratory,': 3, 'Language': 4, 'Natural': 5, 'Processing': 6, 'This': 7, 'branch': 8, 'everyone,': 9, 'is': 10, 'of': 11}\n"
     ]
    }
   ],
   "source": [
    "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
    "print(g_dict1.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 2), (6, 2), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "g_bow =[g_dict1.doc2bow(token, allow_update = True) for token in tokens]\n",
    "print(\"Bag of Words : \", g_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector:\n",
      "[['Artificial', 0.2], ['Hello', 0.2], ['Intelligence', 0.2], ['Laboratory,', 0.2], ['Language', 0.41], ['Natural', 0.41], ['Processing', 0.41], ['This', 0.2], ['branch', 0.2], ['everyone,', 0.2], ['is', 0.41], ['of', 0.2]]\n"
     ]
    }
   ],
   "source": [
    "g_tfidf = models.TfidfModel(g_bow, smartirs='ntc')\n",
    "print(\"TF-IDF Vector:\")\n",
    "for item in g_tfidf[g_bow]:\n",
    "    print([[g_dict1[id], np.around(freq, decimals=2)] for id, freq in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('microcomputer', 0.8632597327232361), ('risc', 0.8498501777648926), ('handheld', 0.841256856918335), ('firmware', 0.8354307413101196), ('workstation', 0.8338199853897095), ('turbo', 0.8318887948989868), ('ipod', 0.829750657081604), ('vax', 0.827304482460022), ('portable', 0.8255654573440552), ('desktop', 0.8245723247528076)]\n"
     ]
    }
   ],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "words = [d for d in dataset]\n",
    "\n",
    "data1 = words[:1000]\n",
    "w2v_model = Word2Vec(data1, min_count = 0, workers=cpu_count())\n",
    "print(w2v_model.wv.most_similar('laptop'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
